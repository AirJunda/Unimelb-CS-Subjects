\documentclass[11pt]{article}
\usepackage{colacl}
\usepackage{multirow}
\sloppy



\title{Spelling Correction Method Evaluation}
%\author
%{Haonan Li \\
%haonanl5@student.unimelb.edu.au}



\begin{document}
\maketitle


\begin{abstract}

This paper investages several spelling correction methods. The main goal is to compare and analysis the performance of spelling correction methods on a peculiar data set, illustrate the characteristic of different string matchind algorithm.

\end{abstract}

\section{Introduction}

Spelling correction is an old NLP task.  From traditional non-neural methods \cite{Hall1980}\cite{Gadd1988â€˜}\cite{Brill2000An}, to neural methods \cite{Han2011}\cite{Eger2016A}\cite{Silfverberg2016Data} presented in recent years. The performance of spelling correction methods have been better and better and become relatively mature. 

In this paper, we investigates some old but really effective and impactful algorithms' performance on spelling correction task. Use a newly UrbanDictionary\footnote{A crowd-compiled dictionary of informal words and slang with over 7 million entries. http://urbandictionary.com} dataset.
The algorithm been evaluated Includ Soundex, N-gram, Edit-distance and Editex.


\section{Methods}

In this section, we simply introduce the algorithms we evaluated in our paper. 

\noindent\textbf{Soundex} was developed by Odell and Russell, and patented in 1918 \cite{Hall1980ApproximateSM}. Uses codes based on the sound of each letter to translate a word into an at most 4 character's string. We call it Soundex code in this paper.

\noindent\textbf{N-Gram} methods are string distance methods based on n-gram counts, where a n-gram of string $s$ is any substring of $s$ of some fixed length. This algorithm defines silimarity of words by calculating n-gram distance, which was proposed by \cite{Ukkonen1992ApproximateSM} defines as:
\begin{equation}
|N_{s}|+|N_{t}|-2|N_{s}N_{t}|
\end{equation}
where $N_{s}$ is the set of n-gram in string $s$. 

\noindent\textbf{Edit distance} presented by \cite{Levenshtein1965Binary},  also know as Levenshtein distance, which is defined as the minimum number of elementary edit operations needed to transform one string into another.

\noindent\textbf{Editex} presented by \cite{Zobel1996PhoneticSM}, is a method combines phonetic matching and edit distance. In this method, alphabets also be grouped similar with soundex. Then, edit distance will be calculated with the grouping info, the distance between two letters in the same group defines 1 while the different groups defins 2. This scheme makes distance between similar phonetic words closer.


\section{Experiments}

\subsection{Data} 

\begin{table}
	\centering
	\begin{tabular}{c|c}
		\hline
		\textbf{Type} & \textbf{Words number} \\
		\hline
		Testset size & 716 \\
		\hline
		Dictionary  size & 393954 \\
		\hline
		Misspelled in Dictionary & 175 \\
		\hline
		Correct not in Dictionary & 122 \\
		\hline
	\end{tabular}
	\caption{Dataset}
	\label{tab:dataset}
\end{table}

For our experiments we use a particular dataset:  a sub-sample of actual data posted to UrbanDictionary, in which a number of headwords taken from have been automatically identified as being misspelled \cite{Saphra2016EvaluatingIW}. This dataset simulates the spelling errer data in the real world as much as possible so that is suitable for our task. Table \ref{tab:dataset} shows the statistics of our dataset.

\subsection{Setting} 

\noindent\textbf{Soundex} Calculate the soundex code for every word and then matched with global edit distance.

\noindent\textbf{N-Gram} We evaluate the N-Gram algorithm for n in range 1 to 9. For a particular $n$, we first pad (n-1) $\sharp$ in the front and end of every word. This gurantee the building of n-gram set. For example, 5-gram set for word ``he'' defined as: 
\begin{equation}
\{\sharp\sharp\sharp\sharp h, \sharp\sharp\sharp he, \sharp\sharp he\sharp, \sharp he\sharp\sharp, he\sharp\sharp\sharp, e\sharp\sharp\sharp\sharp\}
\end{equation}

\noindent\textbf{Edit Distance} There are two kinds of edit distance algorithm, local edit distance and global edit distance. In this paper,  we implement both of them and evaluate them. For global edit distance, we have two distance calculate scheme: 1) (+1) for indel and mismatch and nothing to do for match; 2) (+1) for indel and mismatch and (-1) for match. The different between them will discuss in Section \ref{X}. For local distance algorithm, we use (-1) for indel and mismatch and (+1) for match, and always assign 0 if 0 is better.

\noindent\textbf{Editex} We calculate the editex follows  \cite{Zobel1996PhoneticSM} settings.
 
\section{Results \& Evaluation}

For all experiments in this paper. We keep all parallel best results as the predicted correction. That is, for example, the soundex code for word ``adn'' is ``a35'', while for ``attain'', ``attainabilities'', ``adamas'', ``atom'' etc. the code are the same. No matter how many word in dictionary have the same code with ``adn'', all of them seem equally. Although we know this may result in a bad performance on precision if the predicted set is too large. 

We use this evaluation method because the purpose of this paper is illustrate the characteristics of different string matching alrothrims by spelling correction experiments on a particular dataset, rather than find a best algorithm with parameters to correct the misspelled datasets.\footnote{The dataset we used contains only 716 words, even find best algorithm with parametrs, it might be some kind of overfitting of the particular datasets.} 

\subsection{N-Gram Algorithm Evaluation}

\begin{table}
	\centering
	\small
	\begin{tabular}{c|c|c|c|c}
		\hline
		N &Predicted & Right & Precision & Recall \\
		\hline
		1 & 7150 & 183 & 2.56 & 25.56 \\
		\hline
		2 & 1484 & 151 & 10.19 & 21.09  \\
		\hline
		3 & 1429 & 149 & 10.43 & 20.81 \\
		\hline
		4 & 1426 & 148 & 10.38 & 20.67 \\
		\hline
	\end{tabular}
	\caption{N-gram algorithm results}
	\label{tab:ngram}
\end{table}

The results of n-gram algorithm shows in Table \ref{tab:ngram}, in which we see that when $N=1$, the number of predicted words is large and recall is also the highest. This illustrates that around 25.56\% of the misspelled words are character shifted without deletion and insertion, because the best matches for $N=1$ means two words have the same consist of characters but maybe different order. However, the precision of $N=1$ is really low so it is not a good parameter for this algorithm. $N=2$ and $N=3$ gives more balanced and signicicant resuls. For $N>=4$, the results are all the same. The reason might be that padding $n-1 \sharp$ strategy for n-gram result in the related similarity tend to be table when n is large. 

\subsection{Edit Distance Algorithm Evaluation}

Table \ref{tab:editdis} shows the resuls of edit distance algorithm. We see that even both the first line and second line are results of global edit distance, different distance calculate strategy result in widely divergent results. Both strategy with (+1) for indel and mismatch. The difference is the first do nothing for match while the second (-1). Which always makes the first strategy matchs more result. For example, calculate the edit distance between ``aginst'' with ``against'' and ``agist''. This first strategy's result is the same 1 because ``against'' add one character and ``agist'' delete one character. But the second strategy calculate -5 for ``against'' and -4 for ``agist'', which means ``against'' is more similar with ``against'' because they have more similar elements (characters). Both strategy have its characteristic and we can not say which one is better, it depends on the application scenarious.

As for results of local edit distance algorithm. The predicted words is explosive large but both precision and recall are not satisfactory. Which indicates that this algorithm is not suitable for the task. The reason might be that misspelled words are not tend to be a part of the corresponing correction or in reverse. Nevertheless, we can not say local edit distance is a bad algorithm. It is very powerful in other tasks such as gene alignments.

\begin{table}
	\small
	\centering
	\begin{tabular}{c|c|c|c|c}
		\hline
		Scheme &Predicted & Right & Precision & Recall \\
		\hline
		 GED-1 & 5528 & 253 & 4.57 & 35.34 \\
		\hline
		GED-2 & 2497 & 204 & 8.16 & 28.49 \\
		\hline
		LED & 727774 & 133 & 0.02 & 18.58 \\
		\hline
	\end{tabular}
	\caption{Edit distance algorithm results}
	\label{tab:editdis}
\end{table}


\subsection{Comprehensive   Evaluation}

\begin{table*}
	\centering
	\begin{tabular}{c|c|c|c}
		\hline
		Method & Misspelled & Correct & Matched set \\
		\hline
		\multirow{2}*{Soundex} & accually & actually & akal, axile, azalea, asylees, auxiliar, ... \\
		& ahain & again & awin, annoy, aani, aoyama, anne, ayme, anay, ... \\
		\hline
		\multirow{2}*{Local Edit Distance}  & accually & actually & actually, tactually, unactually, contactually, ...  \\
		& ahain & again & disenchain, rechain, toolchain, toolchains, ... \\
		\hline
		\multirow{2}*{Global Edit Distance}  & accually & actually & actually \\
		& ahain & again & chain, amain, arain, again, ghain, alain, hain  \\
		\hline
		\multirow{2}*{N-Gram (N=2)} & accually & actually & actually, ally \\
		& ahain & again & ain  \\
		\hline
		\multirow{2}*{Editex} & actually & actually & usually, actually, annually, casually, chally, ... \\
		& ahain & again & amain,  attain, arain, again, alain, hain \\
		\hline
	\end{tabular}
	\caption{Demostrate of different algorithm's spelling correction result.}
	\label{tab:match}
\end{table*}

\begin{table*}
	\centering
	\begin{tabular}{c|c|c|c|c}
		\hline
		Method &Predicted & Right & Precision & Recall \\
		\hline
		Soundex & 495146 & 436 & 0.09 & 60.89 \\
		\hline
		Local Edit Distance  & 727774 & 133 & 0.02 & 18.58 \\
		\hline
		Global Edit Distance  & 2497 & 204 & 8.16 & 28.49 \\
		\hline
		N-Gram (N=2) & 1484 & 151 & 10.18 & 21.09 \\
		\hline
		Editex & 2830 & 230 & 8.13 & 32.12 \\
		\hline
	\end{tabular}
	\caption{All method results}
	\label{tab:result}
\end{table*}

Table \ref{tab:match} shows the two words' demostrate output of the best match for each algorithm  and Table \ref{tab:result} shows the results of all evaluation.

From Table \ref{tab:result} we knows that toth soundex and local edit distance predicted too much best matches. The reason for soundex is that for words with a fixed first letter, the number of different soundex code is only 1000, 3 digitals from 0-9. This will lead to plenty of words have the same soundex code.The first two lines of Table \ref{tab:match} demostrate the soundex's result, from which we may find the pronounce of the matched set is somewhat similar with the misspelled word. But the spelling of them are various, which may lead to a number of meaningless predicet.

Although the precision of soundex is extremely low,  it's result seems much better than local edit distance for both precision and recall, which also demostrates in Table \ref{tab:match}.  Obviously, both ``actually'' and any string with a substring is ``actually'' are treated the same. This is the trait of local edit distance, part matching.

As for global edit distance matching, this is a pretty good algorithm for spelling correction that get a acceptable precision and recall. It is not hard to think of computing edis distance based on the soundex results. This is a naive combination of two algorithm which is exceeded by editex. 

We find that editex can get almost the same precision with global edit distance but higher recall. Which might because editex combines the advantage of soundex with edit distance. Form the last two rows of Table \ref{tab:match}, we find that editex matched words with small edit distance and also consider pronounciation.

\section{Conclusions}



\bibliographystyle{acl}
\bibliography{bibliography}

\end{document}
